{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer Learning in DeepDriving Dataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8f1aaa14c8054e59ab5285d85b3c0738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e375298e374444b9c78c83504b469c2",
              "IPY_MODEL_2aaf0019d16f446ab759d6f3802661b6",
              "IPY_MODEL_9cabc4345e3e45fba8e765e2561e11f1"
            ],
            "layout": "IPY_MODEL_5d049dc727ec490e869d9495bd37c9cf"
          }
        },
        "4e375298e374444b9c78c83504b469c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73964566fe7e46b9ad62ce7da9630df2",
            "placeholder": "​",
            "style": "IPY_MODEL_b87ed2c127b3425f83120aae188ce2fa",
            "value": "100%"
          }
        },
        "2aaf0019d16f446ab759d6f3802661b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9fcb5924a7040f1971fe2800ae1677b",
            "max": 77844807,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4252d5f1f3d9407a822fc7094a6a3714",
            "value": 77844807
          }
        },
        "9cabc4345e3e45fba8e765e2561e11f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d6f27bbabb74cfb8093f9de769cf3e2",
            "placeholder": "​",
            "style": "IPY_MODEL_1e2597543bd249c68d31ac15b495c503",
            "value": " 74.2M/74.2M [00:01&lt;00:00, 57.0MB/s]"
          }
        },
        "5d049dc727ec490e869d9495bd37c9cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73964566fe7e46b9ad62ce7da9630df2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b87ed2c127b3425f83120aae188ce2fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9fcb5924a7040f1971fe2800ae1677b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4252d5f1f3d9407a822fc7094a6a3714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d6f27bbabb74cfb8093f9de769cf3e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e2597543bd249c68d31ac15b495c503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### TRANSFER LEARNING IN DeepDriving DATASET\n",
        "\n",
        "Along this notebook, we will use different backbones and pretrained models (FasterRCNN and RetinaNet) to do transfer learning in our dataset. In particular, it is done following the steps:\n",
        "\n",
        "1. Download pretrained model and backbone\n",
        "2. Train this model again using DeepDriving data so the models learn the new labels and specific features of our dataset\n",
        "3. Do inference with these models\n",
        "4. Compare results\n"
      ],
      "metadata": {
        "id": "u-_eQWPT5gZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, we will download the data that will be used and the packages needed to run the whole code"
      ],
      "metadata": {
        "id": "aWIPXh_m6eUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rm -rf DeepDriving"
      ],
      "metadata": {
        "id": "xy_a5_leKEj1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/hemahecodes/AIDL_SelfDrivingProject/raw/dev/transfer_learning/data/deepdriving.zip\n",
        "!unzip deepdriving.zip > /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLPbyKDy6eDT",
        "outputId": "60ff4666-3f6d-42d8-862d-50ff173e2d39"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-15 16:10:59--  https://github.com/hemahecodes/AIDL_SelfDrivingProject/raw/dev/transfer_learning/data/deepdriving.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/hemahecodes/AIDL_SelfDrivingProject/dev/transfer_learning/data/deepdriving.zip [following]\n",
            "--2022-03-15 16:10:59--  https://raw.githubusercontent.com/hemahecodes/AIDL_SelfDrivingProject/dev/transfer_learning/data/deepdriving.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8344272 (8.0M) [application/zip]\n",
            "Saving to: ‘deepdriving.zip.2’\n",
            "\n",
            "deepdriving.zip.2   100%[===================>]   7.96M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-03-15 16:11:00 (58.4 MB/s) - ‘deepdriving.zip.2’ saved [8344272/8344272]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing needed packages\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision import transforms as T\n",
        "import os\n",
        "import json\n",
        "from PIL import Image, ImageDraw\n",
        "from torchvision import transforms\n",
        "from os import listdir\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#If we have GPU, we will use it. Otherwise, not.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "3z_aCVdx79UY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a very first step, we will define the class of DeepDrivingDataset in order to correctly read the data and train the pretrained models."
      ],
      "metadata": {
        "id": "vvucQFsl8YnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We define a class for Berkeley Deep Driving dataset. This class will be specific for training the model because it is in the format that the pretrained model needs.\n",
        "class DeepDrivingDataset(object):\n",
        "    label2idx = {\"other vehicle\": 0,\"person\": 1,\"traffic light\": 2,\"traffic sign\": 3,\"truck\": 4,\"train\": 5,\"other person\": 6,\"bus\": 7,\"car\": 8,\"rider\": 9, \"motor\": 10, \"bike\": 11, \"trailer\": 12}\n",
        "    def __init__(self, train = True):\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.train = train\n",
        "        if self.train:\n",
        "          self.img_dir = os.path.join(\"DeepDriving\",\"train\") #use os.path.join\n",
        "        else:\n",
        "          self.img_dir = os.path.join(\"DeepDriving\",\"val\")\n",
        "        json_file = os.path.join(self.img_dir, \"labels_TL.json\")\n",
        "        with open(json_file) as f:\n",
        "          imgs_anns = json.load(f)\n",
        "\n",
        "        self.imgs = []\n",
        "        self.annotations = []\n",
        "        for idx, v in enumerate(imgs_anns.values()):\n",
        "          filename = os.path.join(self.img_dir, v[\"name\"])\n",
        "          self.imgs.append(filename)\n",
        "          self.annotations.append(v[\"labels\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images\n",
        "        img_path = self.imgs[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # get bounding box coordinates for each object detected\n",
        "        boxes = []\n",
        "        categories = []\n",
        "        for labels in self.annotations[idx]:\n",
        "          if 'box2d' in labels:\n",
        "            annotation = labels['box2d']\n",
        "            lab = labels['category']\n",
        "            categories.append(self.label2idx[lab])\n",
        "            #select the corners of the boxes for each axis. it should be a list with 4 values: 2 coordinates.\n",
        "            boxes.append([annotation[\"x1\"],annotation[\"y1\"],annotation[\"x2\"],annotation[\"y2\"]]) \n",
        "          else:\n",
        "            continue\n",
        "          \n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32, device=device)\n",
        "        boxes.to(device)\n",
        "        labels = torch.tensor(categories, dtype=torch.int64, device=device)\n",
        "        labels.to(device)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "\n",
        "        img = to_tensor(img).to(device)\n",
        "        \n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "def collate_fn(batch):\n",
        "    images = []\n",
        "    targets = []\n",
        "    for i, t in batch:\n",
        "        images.append(i)\n",
        "        targets.append(t)\n",
        "    return images, targets\n",
        "\n"
      ],
      "metadata": {
        "id": "ag9eVfOH8PD7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Faster RCNN, there are 3 different backbones available:\n",
        "1. MobileNetV3\n",
        "2. ResNet50\n",
        "3. MobileNetV3-320\n",
        "\n",
        "So, taking this parameter into account, we will be able to define our **model**. It is important to specify the number of classes (13), so the model is adapted to our data."
      ],
      "metadata": {
        "id": "wjt80t2D9FW2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Aev6k3gJ5ef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "8f1aaa14c8054e59ab5285d85b3c0738",
            "4e375298e374444b9c78c83504b469c2",
            "2aaf0019d16f446ab759d6f3802661b6",
            "9cabc4345e3e45fba8e765e2561e11f1",
            "5d049dc727ec490e869d9495bd37c9cf",
            "73964566fe7e46b9ad62ce7da9630df2",
            "b87ed2c127b3425f83120aae188ce2fa",
            "c9fcb5924a7040f1971fe2800ae1677b",
            "4252d5f1f3d9407a822fc7094a6a3714",
            "4d6f27bbabb74cfb8093f9de769cf3e2",
            "1e2597543bd249c68d31ac15b495c503"
          ]
        },
        "outputId": "b3e80a78-8bf3-49a4-b4cd-b9a2cba7e164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_mobilenet_v3_large_320_fpn-907ea3f9.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_mobilenet_v3_large_320_fpn-907ea3f9.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/74.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f1aaa14c8054e59ab5285d85b3c0738"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#backb will be the backbone used, we will start with MobileNetV3 (backb = 1)\n",
        "#num_epochs will be the number of epochs that we want to use to train the model with DeepDriving data\n",
        "\n",
        "backb = 3\n",
        "num_classes = 13\n",
        "num_epochs = 20\n",
        "if backb == 1:\n",
        "  backbone = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "  backb_used = \"MobileNetV3\"\n",
        "elif backb == 2:\n",
        "  backbone = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "  backb_used = \"ResNet 50\"\n",
        "elif backb == 3:\n",
        "  backbone = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n",
        "  backb_used = \"MobileNetV3-320\"\n",
        "\n",
        "# Now, we can define our model\n",
        "# Function that will give us the model\n",
        "def get_model_object_detection(num_classes):\n",
        "    # load an object detection model pre-trained on COCO\n",
        "    model = backbone\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model \n",
        "\n",
        "# Get the model using our helper function\n",
        "model = get_model_object_detection(num_classes)\n",
        "model = model.to(device) # move model to the right device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have already finished the first step (defining the pretrained model adapted to our dataset), so now we are going to train it with DeepDriving Dataset. \n",
        "\n",
        "In order to do that, we are going to define the **training** loop which will be very easy doing:\n",
        "1. Set optimizer zero grad\n",
        "2. Save loss from model\n",
        "3. Perform backpropagation\n",
        "4. Do an step of the optimizer"
      ],
      "metadata": {
        "id": "ahl48sDo-M9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data_train):\n",
        "  for batch_idx, (img_data, target_data) in enumerate(data_train):\n",
        "      optimizer.zero_grad()\n",
        "      loss_dict = model(img_data, target_data)\n",
        "      loss = sum(loss for loss in loss_dict.values())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if batch_idx%2 == 0:\n",
        "          loss_dict_printable = {k: f\"{v.item():.2f}\" for k, v in loss_dict.items()}\n",
        "          print(f\"[{batch_idx}/{len(data_train)}] loss: {loss_dict_printable}\")\n"
      ],
      "metadata": {
        "id": "kxi1y7lAA-lP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **evaluation** loop will be a little bit more complicated.\n",
        "\n",
        "In this one, we are going to compute the average precision for each epoch. In order to do that we should keep in mind that the average precision is defined as the area beyond the precision-recall curve. So, the steps followed to do this computations are:\n",
        "\n",
        "1. Identify the different boxes predicted and use Non-Maximum-Supresion (nms) with an IoU self-defined (0.2 for example). On this step, the idea is remove the predicted bounding boxes that are overlapping with other predicted bboxes.\n",
        "2. After that, we will loop through every categories and for each category we will:\n",
        "\n",
        "  2.1. Compare the predicted bounding boxes of these categories with the GT ones. If the IoU is higher than our threshold, we will have a TP, otherwise, we will have a FP\n",
        "\n",
        "  2.2. Once a GT box is used (a bbox is used when it has the maximum IoU with a predicted bbox) it cannot be used again, so it should be discarded for the following comparisons\n",
        "\n",
        "  2.3. Finally we will sum up all the TP and all the FP on other side and we will then compute precision and recall"
      ],
      "metadata": {
        "id": "nzwxr3Y7SO3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(data_test):\n",
        "  category_list = [\"other vehicle\", \"person\", \"traffic light\", \"traffic sign\",\"truck\", \"train\", \"other person\", \"bus\", \"car\", \"rider\", \"motor\",\"bike\", \"trailer\"]\n",
        "  # Defining hyperparameters:\n",
        "  hparams = {\n",
        "      'num_epochs': 10,\n",
        "      'batch_size': 5,\n",
        "      'channels': 3,\n",
        "      'learning_rate': 0.0001,\n",
        "      'classes': len(category_list),\n",
        "      'nsamples': 25000,\n",
        "      'grid_size': 14\n",
        "  }\n",
        "  label2idx = {\"other vehicle\": 0,\"person\": 1,\"traffic light\": 2,\"traffic sign\": 3,\"truck\": 4,\"train\": 5,\"other person\": 6,\"bus\": 7,\"car\": 8,\"rider\": 9, \"motor\": 10, \"bike\": 11, \"trailer\": 12}\n",
        "  idx2label = {v: k for k, v in label2idx.items()}\n",
        "  iou_threshold = 0.2\n",
        "  score_threshold = 0.4\n",
        "  total_AP = []\n",
        "  print(\"DATA IS BEING VALIDATED FOR A NEW EPOCH\")\n",
        "  print(\"\")\n",
        "  for batch_idx, (img_data, target_data) in enumerate(data_test):\n",
        "      # img_data = img_data.to(device) #Image loaded, converted as a tensor and resized to 448x448\n",
        "      # target_data = target_data.to(device) #Labels\n",
        "      prediction = model(img_data)\n",
        "      epsilon = 1e-6\n",
        "      \n",
        "      for i in range(hparams['batch_size']):\n",
        "          precisions = [0]*len(category_list)\n",
        "          recalls = [0]*len(category_list)\n",
        "          im_AP = []\n",
        "          im = to_pil_image(img_data[i])\n",
        "          draw = ImageDraw.Draw(im)\n",
        "          classes_target = target_data[i][\"labels\"]\n",
        "          boxes_target = target_data[i][\"boxes\"]\n",
        "          total_boxes_target = len(boxes_target) #Total quantity of bboxes on the GT\n",
        "          true_boxes_used = torch.zeros(total_boxes_target) #We will be checking each bbox used (used means compared with a bbox detected)\n",
        "          true_boxes_counted = torch.zeros(total_boxes_target) #Needed for defining the total number of bbox of a specific class in GT\n",
        "          keep_idx = torchvision.ops.nms(prediction[i]['boxes'], prediction[i]['scores'], iou_threshold) #Performs non-maximum suppression (NMS) on the boxes according to their IoU\n",
        "          #We keep only the predicted bboxes, sxores and labels that we obtain after NMS\n",
        "          boxes = [b for i, b in enumerate(prediction[i][\"boxes\"]) if i in keep_idx] \n",
        "          scores = [s for i, s in enumerate(prediction[i][\"scores\"]) if i in keep_idx]\n",
        "          labels = [l for i, l in enumerate(prediction[i][\"labels\"]) if i in keep_idx]\n",
        "          #Loop by classes in order to compute TP, FP, recall, precision per class\n",
        "          for c in range(len(category_list)):\n",
        "              boxes_pred = []\n",
        "              scores_pred = []\n",
        "              for l in range(len(boxes)):\n",
        "                  if labels[l] == c and scores[l] > score_threshold:\n",
        "                      #Resizing the predictions so they are not on images (448,448) but on the real size\n",
        "                      x1_pred = boxes[l][0]\n",
        "                      x2_pred = boxes[l][2]\n",
        "                      y1_pred = boxes[l][1]\n",
        "                      y2_pred = boxes[l][3]\n",
        "                      box_pred = torch.Tensor([x1_pred,y1_pred, x2_pred, y2_pred])\n",
        "                      boxes_pred.append(box_pred)\n",
        "                      scores_pred.append(scores[l])\n",
        "              \n",
        "              #Each prediction will be a True Positive or a False Positive\n",
        "              TP = torch.zeros((len(boxes_pred)))\n",
        "              FP = torch.zeros((len(boxes_pred)))\n",
        "              total_boxes_target_class = 0\n",
        "\n",
        "              #We loop over the boxes predicted\n",
        "              for det_idx, p in enumerate(boxes_pred):\n",
        "                  iou_max = 0\n",
        "                  #For each box predicted, we will look for the best (highest IoU) GT box and then GT box will be checked as used.\n",
        "                  for idx, t in enumerate(boxes_target):\n",
        "                      if classes_target[idx] == c:\n",
        "                          if true_boxes_counted[idx] == 0:\n",
        "                              total_boxes_target_class = total_boxes_target_class + 1\n",
        "                              true_boxes_counted[idx] = 1\n",
        "                          x1 = torch.max(t[0], p[0])\n",
        "                          y1 = torch.max(t[1], p[1])\n",
        "                          x2 = torch.min(t[2], p[2])\n",
        "                          y2 = torch.min(t[3], p[3])\n",
        "                          # .clamp(0) is for the case when they do not intersect\n",
        "                          intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "\n",
        "                          box1_area = abs((t[2] - t[0]) * (t[3] - t[1]))\n",
        "                          box2_area = abs((p[2] - p[0]) * (p[3] - p[1]))\n",
        "\n",
        "                          iou = intersection / (box1_area + box2_area - intersection + 1e-6)\n",
        "\n",
        "                          if iou >= iou_max:\n",
        "                              iou_max = iou\n",
        "                              true_index = idx\n",
        "                  #If the maximum IoU is greater than the threshold and the GT bbox is not used yet, we have a TP\n",
        "                  if iou_max > iou_threshold:\n",
        "                      if true_boxes_used[idx] == 0:\n",
        "                          TP[det_idx] = 1\n",
        "                          true_boxes_used[true_index] = 1\n",
        "                          coords = p.cpu().tolist()\n",
        "                          draw.rectangle(coords, width = 3) \n",
        "                          text = f\"{idx2label[c]} {scores_pred[det_idx]*100:.2f}%\"\n",
        "                          draw.text([coords[0], coords[1]-15], text)\n",
        "                      else:\n",
        "                          FP[det_idx] = 1\n",
        "                          coords = p.cpu().tolist()\n",
        "                          draw.rectangle(coords, width = 3) \n",
        "                          text = f\"{idx2label[c]} {scores_pred[det_idx]*100:.2f}%\"\n",
        "                          draw.text([coords[0], coords[1]-15], text)\n",
        "                  else:\n",
        "                      FP[det_idx] = 1\n",
        "                      coords = p.cpu().tolist()\n",
        "                      draw.rectangle(coords, width = 3) \n",
        "                      text = f\"{idx2label[c]} {scores_pred[det_idx]*100:.2f}%\"\n",
        "                      draw.text([coords[0], coords[1]-15], text)\n",
        "              if total_boxes_target_class == 0:\n",
        "                  continue\n",
        "              else:\n",
        "                  #Sum of all TP and FP to compute recall and precision for each class\n",
        "                  TP_cumsum = torch.cumsum(TP, dim = 0)\n",
        "                  FP_cumsum = torch.cumsum(FP, dim = 0)\n",
        "                  recalls = TP_cumsum / (total_boxes_target_class + epsilon)\n",
        "                  precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
        "                  precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "                  recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "                  #Average precision is the area under the curve of the precision-recall (we approximate that with trapezoide rule)\n",
        "                  im_AP.append(torch.trapz(precisions, recalls))\n",
        "          if len(im_AP) == 0:\n",
        "            continue\n",
        "          print(\"Average precision of this image: \", sum(im_AP) / len(im_AP))\n",
        "          im.show()\n",
        "          total_AP.append(sum(im_AP) / len(im_AP))\n",
        "\n",
        "  \n",
        "  print(\"Mean Average precision of this epoch: \", np.mean(total_AP) )                        \n"
      ],
      "metadata": {
        "id": "ZBjZL91LQXen"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now that we have already defined the training and evaluating loops, we can use them on the main loop in order to have a control of the loss and mAP epoch by epoch."
      ],
      "metadata": {
        "id": "Ec05Y1lJWZOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer used: SGD\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "# LR scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "def train_eval(json_train, img_train, jsons_p_val, imgs_p_val):\n",
        "    category_list = [\"other vehicle\", \"person\", \"traffic light\", \"traffic sign\",\"truck\", \"train\", \"other person\", \"bus\", \"car\", \"rider\", \"motor\",\"bike\", \"trailer\"]\n",
        "    # Defining hyperparameters:\n",
        "    hparams = {\n",
        "        'num_epochs': 10,\n",
        "        'batch_size': 5,\n",
        "        'channels': 3,\n",
        "        'learning_rate': 0.0001,\n",
        "        'classes': len(category_list),\n",
        "        'nsamples': 25000,\n",
        "        'grid_size': 14\n",
        "    }\n",
        "    use_gpu = True\n",
        "    data_train = DeepDrivingDataset(train=True)\n",
        "    training_dataloader = torch.utils.data.DataLoader(data_train, batch_size=hparams['batch_size'], num_workers=0, collate_fn=collate_fn)\n",
        "\n",
        "    data_test = DeepDrivingDataset(train=False)\n",
        "    testing_dataloader = torch.utils.data.DataLoader(data_test, batch_size=hparams['batch_size'], num_workers=0, collate_fn=collate_fn)\n",
        "    \n",
        "    for epoch in range(hparams['num_epochs']):\n",
        "\n",
        "\n",
        "        total_AP_test = []\n",
        "        print(\"\")\n",
        "        # data_train.LoadFiles()  # Resets the Training DataLoader for a new epoch\n",
        "        # data_test.LoadFiles()  # Resets the Validation DataLoader for a new epoch\n",
        "\n",
        "        model.train()\n",
        "        train(training_dataloader)\n",
        "        model.eval()\n",
        "        evaluate(testing_dataloader)\n",
        "\n",
        "train_eval(\"DeepDriving/train/labels_TL.json\", \"DeepDriving/train/\", \"DeepDriving/val/labels_TL.json\", \"DeepDriving/val/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdlB82G7_ozX",
        "outputId": "abcd080b-a4bf-49c5-e81f-45437048f184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[0/25] loss: {'loss_classifier': '2.58', 'loss_box_reg': '0.37', 'loss_objectness': '0.14', 'loss_rpn_box_reg': '0.20'}\n",
            "[2/25] loss: {'loss_classifier': '1.06', 'loss_box_reg': '0.28', 'loss_objectness': '0.09', 'loss_rpn_box_reg': '0.09'}\n",
            "[4/25] loss: {'loss_classifier': '0.77', 'loss_box_reg': '0.33', 'loss_objectness': '0.12', 'loss_rpn_box_reg': '0.12'}\n",
            "[6/25] loss: {'loss_classifier': '0.76', 'loss_box_reg': '0.30', 'loss_objectness': '0.11', 'loss_rpn_box_reg': '0.16'}\n",
            "[8/25] loss: {'loss_classifier': '0.60', 'loss_box_reg': '0.38', 'loss_objectness': '0.11', 'loss_rpn_box_reg': '0.27'}\n",
            "[10/25] loss: {'loss_classifier': '0.49', 'loss_box_reg': '0.31', 'loss_objectness': '0.11', 'loss_rpn_box_reg': '0.13'}\n",
            "[12/25] loss: {'loss_classifier': '0.51', 'loss_box_reg': '0.38', 'loss_objectness': '0.17', 'loss_rpn_box_reg': '0.19'}\n",
            "[14/25] loss: {'loss_classifier': '0.45', 'loss_box_reg': '0.32', 'loss_objectness': '0.14', 'loss_rpn_box_reg': '0.16'}\n",
            "[16/25] loss: {'loss_classifier': '0.35', 'loss_box_reg': '0.17', 'loss_objectness': '0.11', 'loss_rpn_box_reg': '0.11'}\n",
            "[18/25] loss: {'loss_classifier': '0.31', 'loss_box_reg': '0.25', 'loss_objectness': '0.08', 'loss_rpn_box_reg': '0.10'}\n",
            "[20/25] loss: {'loss_classifier': '0.34', 'loss_box_reg': '0.20', 'loss_objectness': '0.09', 'loss_rpn_box_reg': '0.17'}\n",
            "[22/25] loss: {'loss_classifier': '0.32', 'loss_box_reg': '0.14', 'loss_objectness': '0.10', 'loss_rpn_box_reg': '0.14'}\n",
            "[24/25] loss: {'loss_classifier': '0.45', 'loss_box_reg': '0.28', 'loss_objectness': '0.12', 'loss_rpn_box_reg': '0.25'}\n",
            "DATA IS BEING VALIDATED FOR A NEW EPOCH\n",
            "\n",
            "Average precision of this image:  tensor(0.4509)\n",
            "Average precision of this image:  tensor(0.1250)\n",
            "Average precision of this image:  tensor(0.5556)\n",
            "Average precision of this image:  tensor(0.4545)\n",
            "Average precision of this image:  tensor(0.1875)\n",
            "Average precision of this image:  tensor(0.3500)\n",
            "Average precision of this image:  tensor(0.2273)\n",
            "Average precision of this image:  tensor(0.5040)\n",
            "Average precision of this image:  tensor(1.0000)\n",
            "Average precision of this image:  tensor(0.2000)\n",
            "Average precision of this image:  tensor(0.2500)\n",
            "Average precision of this image:  tensor(0.2000)\n",
            "Average precision of this image:  tensor(0.1250)\n",
            "Average precision of this image:  tensor(0.1250)\n",
            "Average precision of this image:  tensor(0.0833)\n",
            "Average precision of this image:  tensor(0.1637)\n",
            "Average precision of this image:  tensor(0.1667)\n",
            "Average precision of this image:  tensor(0.1403)\n",
            "Average precision of this image:  tensor(0.3849)\n",
            "Mean Average precision of this epoch:  0.29966903\n",
            "\n",
            "[0/25] loss: {'loss_classifier': '0.55', 'loss_box_reg': '0.31', 'loss_objectness': '0.07', 'loss_rpn_box_reg': '0.21'}\n",
            "[2/25] loss: {'loss_classifier': '0.28', 'loss_box_reg': '0.21', 'loss_objectness': '0.07', 'loss_rpn_box_reg': '0.09'}\n",
            "[4/25] loss: {'loss_classifier': '0.29', 'loss_box_reg': '0.25', 'loss_objectness': '0.10', 'loss_rpn_box_reg': '0.11'}\n",
            "[6/25] loss: {'loss_classifier': '0.25', 'loss_box_reg': '0.19', 'loss_objectness': '0.09', 'loss_rpn_box_reg': '0.12'}\n",
            "[8/25] loss: {'loss_classifier': '0.51', 'loss_box_reg': '0.30', 'loss_objectness': '0.10', 'loss_rpn_box_reg': '0.24'}\n",
            "[10/25] loss: {'loss_classifier': '0.31', 'loss_box_reg': '0.29', 'loss_objectness': '0.08', 'loss_rpn_box_reg': '0.10'}\n",
            "[12/25] loss: {'loss_classifier': '0.44', 'loss_box_reg': '0.36', 'loss_objectness': '0.12', 'loss_rpn_box_reg': '0.17'}\n",
            "[14/25] loss: {'loss_classifier': '0.41', 'loss_box_reg': '0.31', 'loss_objectness': '0.12', 'loss_rpn_box_reg': '0.14'}\n",
            "[16/25] loss: {'loss_classifier': '0.35', 'loss_box_reg': '0.20', 'loss_objectness': '0.08', 'loss_rpn_box_reg': '0.10'}\n",
            "[18/25] loss: {'loss_classifier': '0.30', 'loss_box_reg': '0.31', 'loss_objectness': '0.04', 'loss_rpn_box_reg': '0.10'}\n"
          ]
        }
      ]
    }
  ]
}