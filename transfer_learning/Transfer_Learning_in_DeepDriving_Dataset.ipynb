{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-_eQWPT5gZI"
      },
      "source": [
        "### TRANSFER LEARNING IN DeepDriving DATASET\n",
        "\n",
        "Along this notebook, we will use different backbones and pretrained models (FasterRCNN and RetinaNet) to do transfer learning in our dataset. In particular, it is done following the steps:\n",
        "\n",
        "1. Download pretrained model and backbone\n",
        "2. Train this model again using DeepDriving data so the models learn the new labels and specific features of our dataset\n",
        "3. Do inference with these models\n",
        "4. Compare results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1. Prepare environment\n",
        "\n"
      ],
      "metadata": {
        "id": "JgUlkOzxBIkW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWIPXh_m6eUV"
      },
      "source": [
        "The preparation of the environment consists of download and unzip the data needed for training and validating the models that we will create. Also, another step on this first section will be importing the needed packages for running the whole code without packages errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLPbyKDy6eDT",
        "outputId": "c6f2dea2-8d7e-41bf-c2f8-f905734d1e03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-16 05:23:51--  https://github.com/hemahecodes/AIDL_SelfDrivingProject/raw/dev/transfer_learning/data/deepdriving.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/hemahecodes/AIDL_SelfDrivingProject/dev/transfer_learning/data/deepdriving.zip [following]\n",
            "--2022-03-16 05:23:52--  https://raw.githubusercontent.com/hemahecodes/AIDL_SelfDrivingProject/dev/transfer_learning/data/deepdriving.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8344272 (8.0M) [application/zip]\n",
            "Saving to: ‘deepdriving.zip’\n",
            "\n",
            "deepdriving.zip     100%[===================>]   7.96M  49.2MB/s    in 0.2s    \n",
            "\n",
            "2022-03-16 05:23:52 (49.2 MB/s) - ‘deepdriving.zip’ saved [8344272/8344272]\n",
            "\n",
            "replace DeepDriving/train/4c2cd55b-31488766.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/hemahecodes/AIDL_SelfDrivingProject/raw/main/transfer_learning/data/deepdriving.zip\n",
        "!unzip deepdriving.zip > /dev/null\n",
        "!rm -rf deepdriving.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3z_aCVdx79UY"
      },
      "outputs": [],
      "source": [
        "#Importing needed packages\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision import transforms as T\n",
        "import os\n",
        "import json\n",
        "from PIL import Image, ImageDraw\n",
        "from torchvision import transforms\n",
        "from os import listdir\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "#If we have GPU, we will use it. Otherwise, not.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Define DeepDriving Dataset Class"
      ],
      "metadata": {
        "id": "rKNus-CdBRuT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvucQFsl8YnI"
      },
      "source": [
        "As a very first step of the main problem, we will define the class of DeepDrivingDataset in order to correctly read the data and *retrain* the pretrained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ag9eVfOH8PD7"
      },
      "outputs": [],
      "source": [
        "#We define a class for Berkeley Deep Driving dataset. This class will be specific for training the model because it is in the format that the pretrained model needs.\n",
        "class DeepDrivingDataset(object):\n",
        "    label2idx = {\"other vehicle\": 0,\"person\": 1,\"traffic light\": 2,\"traffic sign\": 3,\"truck\": 4,\"train\": 5,\"other person\": 6,\"bus\": 7,\"car\": 8,\"rider\": 9, \"motor\": 10, \"bike\": 11, \"trailer\": 12}\n",
        "    def __init__(self, train = True):\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.train = train\n",
        "        if self.train:\n",
        "          self.img_dir = os.path.join(\"DeepDriving\",\"train\") #use os.path.join\n",
        "        else:\n",
        "          self.img_dir = os.path.join(\"DeepDriving\",\"val\")\n",
        "        json_file = os.path.join(self.img_dir, \"labels_TL.json\")\n",
        "        with open(json_file) as f:\n",
        "          imgs_anns = json.load(f)\n",
        "\n",
        "        self.imgs = []\n",
        "        self.annotations = []\n",
        "        for idx, v in enumerate(imgs_anns.values()):\n",
        "          filename = os.path.join(self.img_dir, v[\"name\"])\n",
        "          self.imgs.append(filename)\n",
        "          self.annotations.append(v[\"labels\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images\n",
        "        img_path = self.imgs[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # get bounding box coordinates for each object detected\n",
        "        boxes = []\n",
        "        categories = []\n",
        "        for labels in self.annotations[idx]:\n",
        "          if 'box2d' in labels:\n",
        "            annotation = labels['box2d']\n",
        "            lab = labels['category']\n",
        "            categories.append(self.label2idx[lab])\n",
        "            #select the corners of the boxes for each axis. it should be a list with 4 values: 2 coordinates.\n",
        "            boxes.append([annotation[\"x1\"],annotation[\"y1\"],annotation[\"x2\"],annotation[\"y2\"]]) \n",
        "          else:\n",
        "            continue\n",
        "          \n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32, device=device)\n",
        "        boxes.to(device)\n",
        "        labels = torch.tensor(categories, dtype=torch.int64, device=device)\n",
        "        labels.to(device)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "\n",
        "        img = to_tensor(img).to(device)\n",
        "        \n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "def collate_fn(batch):\n",
        "    images = []\n",
        "    targets = []\n",
        "    for i, t in batch:\n",
        "        images.append(i)\n",
        "        targets.append(t)\n",
        "    return images, targets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Download the Pretrained model"
      ],
      "metadata": {
        "id": "VcDcN7HCBazU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjt80t2D9FW2"
      },
      "source": [
        "We will use Pretrained Faster RCNN and RetinaNet. In order to choose the model we will use the parameter *pret_model = FastRCNN* or *pret_model = RetinaNet*.\n",
        "\n",
        "For Faster RCNN, there are 3 different backbones available:\n",
        "1. MobileNetV3\n",
        "2. ResNet50\n",
        "3. MobileNetV3-320\n",
        "\n",
        "This last parameter will be called with *backb = 1*, *backb = 2* or *backb = 3*, respectively  \n",
        "\n",
        "So, taking these details into account, we will be able to define our **model**. It is very important to specify the number of classes (13), so the model is adapted to our data. If we do not do this, the model will not be able to train with our data because the number of classes will be different and so the labels won't be learnt by the new model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Aev6k3gJ5ef6"
      },
      "outputs": [],
      "source": [
        "#backb will be the backbone used, we will start with MobileNetV3 (backb = 1)\n",
        "backb = 1\n",
        "pret_model = \"FasterRCNN\" #for using FasterRCNN Pretrained model\n",
        "# pret_model = \"RetinaNet\" #for using RetinaNet Pretrained model\n",
        "\n",
        "# Now, we can define our model\n",
        "# Function that will give us the model\n",
        "def get_model_object_detection_fasterrcnn(num_classes):\n",
        "    # load an object detection model pre-trained on COCO\n",
        "    model = backbone\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model \n",
        "\n",
        "#Function that will give us the model\n",
        "def get_model_object_detection_retinanet(num_classes):\n",
        "  backbone = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
        "  backbone.out_channels = 2048\n",
        "  backb_used = \"ResNet 50\"\n",
        "\n",
        "  # Now, we make the RPN generate 5x3 anchors per spatial location (5 different sizes, 3 different aspect ratios)\n",
        "  anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "  roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],output_size=7,sampling_ratio=2)\n",
        "\n",
        "  # load an object detection model pre-trained on COCO\n",
        "  model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
        "  # get number of input features for the classifier\n",
        "  in_features = model.head.classification_head.conv[0].in_channels\n",
        "  num_anchors = model.head.classification_head.num_anchors\n",
        "  model.head.classification_head.num_classes = num_classes\n",
        "\n",
        "  cls_logits = torch.nn.Conv2d(256, num_anchors * num_classes, kernel_size = 3, stride=1, padding=1)\n",
        "  torch.nn.init.normal_(cls_logits.weight, std=0.01)  # as per pytorch code\n",
        "  torch.nn.init.constant_(cls_logits.bias, -math.log((1 - 0.01) / 0.01))  # as per pytorcch code \n",
        "  # assign cls head to model\n",
        "  model.head.classification_head.cls_logits = cls_logits\n",
        "\n",
        "  return model \n",
        "\n",
        "num_classes = 13\n",
        "if backb == 1 and pret_model == \"FasterRCNN\":\n",
        "  backbone = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "  backb_used = \"MobileNetV3\"\n",
        "  model = get_model_object_detection_fasterrcnn(num_classes)\n",
        "  name_model = backb_used + \" in FasterRCNN\"\n",
        "elif backb == 2 and pret_model == \"FasterRCNN\":\n",
        "  backbone = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "  backb_used = \"ResNet 50\"\n",
        "  model = get_model_object_detection_fasterrcnn(num_classes)\n",
        "  name_model = backb_used + \" in FasterRCNN\"\n",
        "elif backb == 3 and pret_model == \"FasterRCNN\":\n",
        "  backbone = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n",
        "  backb_used = \"MobileNetV3-320\"\n",
        "  model = get_model_object_detection_fasterrcnn(num_classes)\n",
        "  name_model = backb_used + \" in FasterRCNN\"\n",
        "elif pret_model == \"RetinaNet\":\n",
        "  model = get_model_object_detection_retinanet(num_classes)\n",
        "  name_model = \"ResNet50 in RetinaNet\"\n",
        "\n",
        "# move model to the right device\n",
        "model = model.to(device) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Create the training function"
      ],
      "metadata": {
        "id": "zgZkdgIFCJkY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahl48sDo-M9N"
      },
      "source": [
        "We have already finished the first step (defining the pretrained model adapted to our dataset), so now we are going to train it with DeepDriving Dataset. \n",
        "\n",
        "In order to do that, we are going to define the **training** loop which will be very easy doing:\n",
        "1. Set optimizer gradients to zero\n",
        "2. Save loss from model\n",
        "3. Perform backpropagation\n",
        "4. Do an step of the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kxi1y7lAA-lP"
      },
      "outputs": [],
      "source": [
        "def train(data_train):\n",
        "  for batch_idx, (img_data, target_data) in enumerate(data_train):\n",
        "      optimizer.zero_grad()\n",
        "      loss_dict = model(img_data, target_data)\n",
        "      loss = sum(loss for loss in loss_dict.values())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if batch_idx%2 == 0:\n",
        "          loss_dict_printable = {k: f\"{v.item():.2f}\" for k, v in loss_dict.items()}\n",
        "          print(f\"[{batch_idx}/{len(data_train)}] loss: {loss_dict_printable}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Create the evaluation function"
      ],
      "metadata": {
        "id": "YUJ9HFihCVtO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzwxr3Y7SO3d"
      },
      "source": [
        "The **evaluation** function will be a little bit more complicated.\n",
        "\n",
        "In this one, we are going to compute the average precision for each epoch. In order to do that we should keep in mind that the average precision is defined as the area beyond the precision-recall curve. So, the steps followed to do this computations are:\n",
        "\n",
        "1. Identify the different boxes predicted and use Non-Maximum-Supresion (nms) with an IoU self-defined (0.2 for example). On this step, the idea is remove the predicted bounding boxes that are overlapping with other predicted bboxes.\n",
        "2. After that, we will loop through every categories and for each category we will:\n",
        "\n",
        "  2.1. Compare the predicted bounding boxes of these categories with the GT ones. If the IoU is higher than our threshold, we will have a TP, otherwise, we will have a FP\n",
        "\n",
        "  2.2. Once a GT box is used (a bbox is used when it has the maximum IoU with a predicted bbox) it cannot be used again, so it should be discarded for the following comparisons\n",
        "\n",
        "  2.3. Finally we will sum up all the TP and all the FP on other side and we will then compute precision and recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZBjZL91LQXen"
      },
      "outputs": [],
      "source": [
        "def evaluate(data_test):\n",
        "  category_list = [\"other vehicle\", \"person\", \"traffic light\", \"traffic sign\",\"truck\", \"train\", \"other person\", \"bus\", \"car\", \"rider\", \"motor\",\"bike\", \"trailer\"]\n",
        "  # Defining hyperparameters:\n",
        "  hparams = {\n",
        "      'num_epochs': 3,\n",
        "      'batch_size': 5,\n",
        "      'channels': 3,\n",
        "      'learning_rate': 0.0001,\n",
        "      'classes': len(category_list),\n",
        "      'nsamples': 25000,\n",
        "      'grid_size': 14\n",
        "  }\n",
        "  label2idx = {\"other vehicle\": 0,\"person\": 1,\"traffic light\": 2,\"traffic sign\": 3,\"truck\": 4,\"train\": 5,\"other person\": 6,\"bus\": 7,\"car\": 8,\"rider\": 9, \"motor\": 10, \"bike\": 11, \"trailer\": 12}\n",
        "  idx2label = {v: k for k, v in label2idx.items()}\n",
        "  iou_threshold = 0.2\n",
        "  score_threshold = 0.4\n",
        "  total_AP = []\n",
        "  print(\"DATA IS BEING VALIDATED FOR A NEW EPOCH\")\n",
        "  print(\"\")\n",
        "  img_number = 0\n",
        "  for batch_idx, (img_data, target_data) in enumerate(data_test):\n",
        "      prediction = model(img_data)\n",
        "      epsilon = 1e-6\n",
        "      \n",
        "      for i in range(hparams['batch_size']):\n",
        "          precisions = [0]*len(category_list)\n",
        "          recalls = [0]*len(category_list)\n",
        "          im_AP = []\n",
        "          im = to_pil_image(img_data[i])\n",
        "          draw = ImageDraw.Draw(im)\n",
        "          classes_target = target_data[i][\"labels\"]\n",
        "          boxes_target = target_data[i][\"boxes\"]\n",
        "          total_boxes_target = len(boxes_target) #Total quantity of bboxes on the GT\n",
        "          true_boxes_used = torch.zeros(total_boxes_target) #We will be checking each bbox used (used means compared with a bbox detected)\n",
        "          true_boxes_counted = torch.zeros(total_boxes_target) #Needed for defining the total number of bbox of a specific class in GT\n",
        "          keep_idx = torchvision.ops.nms(prediction[i]['boxes'], prediction[i]['scores'], iou_threshold) #Performs non-maximum suppression (NMS) on the boxes according to their IoU\n",
        "          #We keep only the predicted bboxes, sxores and labels that we obtain after NMS\n",
        "          boxes = [b for i, b in enumerate(prediction[i][\"boxes\"]) if i in keep_idx] \n",
        "          scores = [s for i, s in enumerate(prediction[i][\"scores\"]) if i in keep_idx]\n",
        "          labels = [l for i, l in enumerate(prediction[i][\"labels\"]) if i in keep_idx]\n",
        "          #Loop by classes in order to compute TP, FP, recall, precision per class\n",
        "          for c in range(len(category_list)):\n",
        "              boxes_pred = []\n",
        "              scores_pred = []\n",
        "              for l in range(len(boxes)):\n",
        "                  if labels[l] == c and scores[l] > score_threshold:\n",
        "                      #Resizing the predictions so they are not on images (448,448) but on the real size\n",
        "                      x1_pred = boxes[l][0]\n",
        "                      x2_pred = boxes[l][2]\n",
        "                      y1_pred = boxes[l][1]\n",
        "                      y2_pred = boxes[l][3]\n",
        "                      box_pred = torch.Tensor([x1_pred,y1_pred, x2_pred, y2_pred])\n",
        "                      boxes_pred.append(box_pred)\n",
        "                      scores_pred.append(scores[l])\n",
        "              \n",
        "              #Each prediction will be a True Positive or a False Positive\n",
        "              TP = torch.zeros((len(boxes_pred)))\n",
        "              FP = torch.zeros((len(boxes_pred)))\n",
        "              total_boxes_target_class = 0\n",
        "              \n",
        "              #We loop over the boxes predicted\n",
        "              for det_idx, p in enumerate(boxes_pred):\n",
        "                  iou_max = 0\n",
        "                  #For each box predicted, we will look for the best (highest IoU) GT box and then GT box will be checked as used.\n",
        "                  for idx, t in enumerate(boxes_target):\n",
        "                      if classes_target[idx] == c:\n",
        "                          if true_boxes_counted[idx] == 0:\n",
        "                              total_boxes_target_class = total_boxes_target_class + 1\n",
        "                              true_boxes_counted[idx] = 1\n",
        "                          x1 = torch.max(t[0], p[0])\n",
        "                          y1 = torch.max(t[1], p[1])\n",
        "                          x2 = torch.min(t[2], p[2])\n",
        "                          y2 = torch.min(t[3], p[3])\n",
        "                          # .clamp(0) is for the case when they do not intersect\n",
        "                          intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "\n",
        "                          box1_area = abs((t[2] - t[0]) * (t[3] - t[1]))\n",
        "                          box2_area = abs((p[2] - p[0]) * (p[3] - p[1]))\n",
        "\n",
        "                          iou = intersection / (box1_area + box2_area - intersection + 1e-6)\n",
        "\n",
        "                          if iou >= iou_max:\n",
        "                              iou_max = iou\n",
        "                              true_index = idx\n",
        "                  #If the maximum IoU is greater than the threshold and the GT bbox is not used yet, we have a TP\n",
        "                  if iou_max > iou_threshold:\n",
        "                      if true_boxes_used[idx] == 0:\n",
        "                          TP[det_idx] = 1\n",
        "                          true_boxes_used[true_index] = 1\n",
        "                          coords = p.cpu().tolist()\n",
        "                          draw.rectangle(coords, width = 3, outline = \"blue\") \n",
        "                          text = f\"{idx2label[c]} {scores_pred[det_idx]*100:.2f}%\"\n",
        "                          draw.text([coords[0], coords[1]-15], text)\n",
        "                      else:\n",
        "                          FP[det_idx] = 1\n",
        "                          coords = p.cpu().tolist()\n",
        "                          draw.rectangle(coords, width = 3, outline = \"blue\") \n",
        "                          text = f\"{idx2label[c]} {scores_pred[det_idx]*100:.2f}%\"\n",
        "                          draw.text([coords[0], coords[1]-15], text)\n",
        "                  else:\n",
        "                      FP[det_idx] = 1\n",
        "                      coords = p.cpu().tolist()\n",
        "                      draw.rectangle(coords, width = 3, outline = \"blue\") \n",
        "                      text = f\"{idx2label[c]} {scores_pred[det_idx]*100:.2f}%\"\n",
        "                      draw.text([coords[0], coords[1]-15], text)\n",
        "              if total_boxes_target_class == 0:\n",
        "                  continue\n",
        "              else:\n",
        "                  #Sum of all TP and FP to compute recall and precision for each class\n",
        "                  TP_cumsum = torch.cumsum(TP, dim = 0)\n",
        "                  FP_cumsum = torch.cumsum(FP, dim = 0)\n",
        "                  recalls = TP_cumsum / (total_boxes_target_class + epsilon)\n",
        "                  precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
        "                  precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "                  recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "                  #Average precision is the area under the curve of the precision-recall (we approximate that with trapezoide rule)\n",
        "                  im_AP.append(torch.trapz(precisions, recalls))\n",
        "          if len(im_AP) == 0:\n",
        "            continue\n",
        "          for GTbox in boxes_target:\n",
        "              coords = GTbox.cpu().tolist()\n",
        "              draw.rectangle(coords, width = 3, outline = \"green\") \n",
        "\n",
        "          print(\"Average precision of this image: \", sum(im_AP) / len(im_AP))\n",
        "          img_name = \"predictions/prediction_\" + name_model + str(img_number) + \".png\"\n",
        "          im = im.save(img_name)\n",
        "          img_number = img_number + 1\n",
        "          total_AP.append(sum(im_AP) / len(im_AP))\n",
        "\n",
        "  \n",
        "  print(\"Mean Average precision of this epoch: \", np.mean(total_AP) )                        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Training Loop (with evaluation included)"
      ],
      "metadata": {
        "id": "Rqw4O1PKCdGH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec05Y1lJWZOQ"
      },
      "source": [
        "And now that we have already defined the training and evaluating loops, we can use them on the main loop in order to have a control of the loss and mAP epoch by epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdlB82G7_ozX",
        "outputId": "465c24fe-7415-4809-dc29-7830e5b1262b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0/25] loss: {'loss_classifier': '3.00', 'loss_box_reg': '0.61', 'loss_objectness': '0.35', 'loss_rpn_box_reg': '0.21'}\n",
            "[2/25] loss: {'loss_classifier': '1.30', 'loss_box_reg': '0.63', 'loss_objectness': '0.09', 'loss_rpn_box_reg': '0.10'}\n",
            "[4/25] loss: {'loss_classifier': '0.89', 'loss_box_reg': '0.64', 'loss_objectness': '0.22', 'loss_rpn_box_reg': '0.08'}\n",
            "[6/25] loss: {'loss_classifier': '0.86', 'loss_box_reg': '0.59', 'loss_objectness': '0.16', 'loss_rpn_box_reg': '0.12'}\n",
            "[8/25] loss: {'loss_classifier': '0.76', 'loss_box_reg': '0.51', 'loss_objectness': '0.11', 'loss_rpn_box_reg': '0.18'}\n",
            "[10/25] loss: {'loss_classifier': '0.66', 'loss_box_reg': '0.64', 'loss_objectness': '0.10', 'loss_rpn_box_reg': '0.10'}\n",
            "[12/25] loss: {'loss_classifier': '0.66', 'loss_box_reg': '0.65', 'loss_objectness': '0.14', 'loss_rpn_box_reg': '0.14'}\n",
            "[14/25] loss: {'loss_classifier': '0.58', 'loss_box_reg': '0.67', 'loss_objectness': '0.13', 'loss_rpn_box_reg': '0.08'}\n",
            "[16/25] loss: {'loss_classifier': '0.48', 'loss_box_reg': '0.43', 'loss_objectness': '0.09', 'loss_rpn_box_reg': '0.08'}\n",
            "[18/25] loss: {'loss_classifier': '0.37', 'loss_box_reg': '0.43', 'loss_objectness': '0.08', 'loss_rpn_box_reg': '0.07'}\n",
            "[20/25] loss: {'loss_classifier': '0.40', 'loss_box_reg': '0.39', 'loss_objectness': '0.06', 'loss_rpn_box_reg': '0.11'}\n",
            "[22/25] loss: {'loss_classifier': '0.53', 'loss_box_reg': '0.39', 'loss_objectness': '0.18', 'loss_rpn_box_reg': '0.11'}\n",
            "[24/25] loss: {'loss_classifier': '0.49', 'loss_box_reg': '0.50', 'loss_objectness': '0.11', 'loss_rpn_box_reg': '0.21'}\n",
            "DATA IS BEING VALIDATED FOR A NEW EPOCH\n",
            "\n",
            "Average precision of this image:  tensor(0.4663)\n",
            "Average precision of this image:  tensor(0.3187)\n",
            "Average precision of this image:  tensor(0.6667)\n",
            "Average precision of this image:  tensor(0.5935)\n",
            "Average precision of this image:  tensor(0.4375)\n",
            "Average precision of this image:  tensor(0.6500)\n",
            "Average precision of this image:  tensor(0.2132)\n",
            "Average precision of this image:  tensor(0.1979)\n",
            "Average precision of this image:  tensor(1.0000)\n",
            "Average precision of this image:  tensor(0.6313)\n",
            "Average precision of this image:  tensor(0.3750)\n",
            "Average precision of this image:  tensor(0.2708)\n",
            "Average precision of this image:  tensor(0.2500)\n",
            "Average precision of this image:  tensor(0.2500)\n",
            "Average precision of this image:  tensor(0.1667)\n",
            "Average precision of this image:  tensor(0.1101)\n",
            "Average precision of this image:  tensor(1.0000)\n",
            "Average precision of this image:  tensor(0.2204)\n",
            "Average precision of this image:  tensor(0.4444)\n",
            "Average precision of this image:  tensor(0.)\n",
            "Mean Average precision of this epoch:  0.41312543\n",
            "\n",
            "[0/25] loss: {'loss_classifier': '0.62', 'loss_box_reg': '0.54', 'loss_objectness': '0.06', 'loss_rpn_box_reg': '0.18'}\n",
            "[2/25] loss: {'loss_classifier': '0.34', 'loss_box_reg': '0.41', 'loss_objectness': '0.07', 'loss_rpn_box_reg': '0.10'}\n",
            "[4/25] loss: {'loss_classifier': '0.38', 'loss_box_reg': '0.48', 'loss_objectness': '0.09', 'loss_rpn_box_reg': '0.07'}\n",
            "[6/25] loss: {'loss_classifier': '0.28', 'loss_box_reg': '0.36', 'loss_objectness': '0.06', 'loss_rpn_box_reg': '0.11'}\n",
            "[8/25] loss: {'loss_classifier': '0.54', 'loss_box_reg': '0.49', 'loss_objectness': '0.13', 'loss_rpn_box_reg': '0.16'}\n",
            "[10/25] loss: {'loss_classifier': '0.28', 'loss_box_reg': '0.47', 'loss_objectness': '0.05', 'loss_rpn_box_reg': '0.09'}\n",
            "[12/25] loss: {'loss_classifier': '0.42', 'loss_box_reg': '0.53', 'loss_objectness': '0.10', 'loss_rpn_box_reg': '0.13'}\n",
            "[14/25] loss: {'loss_classifier': '0.45', 'loss_box_reg': '0.54', 'loss_objectness': '0.09', 'loss_rpn_box_reg': '0.09'}\n",
            "[16/25] loss: {'loss_classifier': '0.36', 'loss_box_reg': '0.45', 'loss_objectness': '0.06', 'loss_rpn_box_reg': '0.07'}\n",
            "[18/25] loss: {'loss_classifier': '0.29', 'loss_box_reg': '0.37', 'loss_objectness': '0.05', 'loss_rpn_box_reg': '0.07'}\n",
            "[20/25] loss: {'loss_classifier': '0.37', 'loss_box_reg': '0.40', 'loss_objectness': '0.05', 'loss_rpn_box_reg': '0.10'}\n",
            "[22/25] loss: {'loss_classifier': '0.37', 'loss_box_reg': '0.42', 'loss_objectness': '0.06', 'loss_rpn_box_reg': '0.10'}\n",
            "[24/25] loss: {'loss_classifier': '0.33', 'loss_box_reg': '0.39', 'loss_objectness': '0.04', 'loss_rpn_box_reg': '0.18'}\n",
            "DATA IS BEING VALIDATED FOR A NEW EPOCH\n",
            "\n",
            "Average precision of this image:  tensor(0.4175)\n",
            "Average precision of this image:  tensor(0.2500)\n",
            "Average precision of this image:  tensor(0.5556)\n",
            "Average precision of this image:  tensor(0.6959)\n",
            "Average precision of this image:  tensor(0.5000)\n",
            "Average precision of this image:  tensor(0.7500)\n",
            "Average precision of this image:  tensor(0.4545)\n",
            "Average precision of this image:  tensor(0.1771)\n",
            "Average precision of this image:  tensor(1.0000)\n",
            "Average precision of this image:  tensor(0.7185)\n",
            "Average precision of this image:  tensor(0.3750)\n",
            "Average precision of this image:  tensor(0.4592)\n",
            "Average precision of this image:  tensor(0.2500)\n",
            "Average precision of this image:  tensor(0.1771)\n",
            "Average precision of this image:  tensor(0.2785)\n",
            "Average precision of this image:  tensor(0.5911)\n",
            "Average precision of this image:  tensor(0.2500)\n",
            "Average precision of this image:  tensor(0.1570)\n",
            "Average precision of this image:  tensor(0.4444)\n",
            "Mean Average precision of this epoch:  0.4474425\n",
            "\n",
            "[0/25] loss: {'loss_classifier': '0.51', 'loss_box_reg': '0.49', 'loss_objectness': '0.06', 'loss_rpn_box_reg': '0.16'}\n",
            "[2/25] loss: {'loss_classifier': '0.29', 'loss_box_reg': '0.37', 'loss_objectness': '0.04', 'loss_rpn_box_reg': '0.09'}\n",
            "[4/25] loss: {'loss_classifier': '0.34', 'loss_box_reg': '0.47', 'loss_objectness': '0.09', 'loss_rpn_box_reg': '0.07'}\n",
            "[6/25] loss: {'loss_classifier': '0.25', 'loss_box_reg': '0.35', 'loss_objectness': '0.05', 'loss_rpn_box_reg': '0.10'}\n",
            "[8/25] loss: {'loss_classifier': '0.45', 'loss_box_reg': '0.47', 'loss_objectness': '0.09', 'loss_rpn_box_reg': '0.15'}\n",
            "[10/25] loss: {'loss_classifier': '0.26', 'loss_box_reg': '0.39', 'loss_objectness': '0.05', 'loss_rpn_box_reg': '0.09'}\n",
            "[12/25] loss: {'loss_classifier': '0.39', 'loss_box_reg': '0.52', 'loss_objectness': '0.07', 'loss_rpn_box_reg': '0.12'}\n",
            "[14/25] loss: {'loss_classifier': '0.40', 'loss_box_reg': '0.51', 'loss_objectness': '0.07', 'loss_rpn_box_reg': '0.08'}\n",
            "[16/25] loss: {'loss_classifier': '0.31', 'loss_box_reg': '0.48', 'loss_objectness': '0.05', 'loss_rpn_box_reg': '0.07'}\n",
            "[18/25] loss: {'loss_classifier': '0.23', 'loss_box_reg': '0.32', 'loss_objectness': '0.04', 'loss_rpn_box_reg': '0.07'}\n",
            "[20/25] loss: {'loss_classifier': '0.32', 'loss_box_reg': '0.39', 'loss_objectness': '0.04', 'loss_rpn_box_reg': '0.09'}\n",
            "[22/25] loss: {'loss_classifier': '0.39', 'loss_box_reg': '0.44', 'loss_objectness': '0.05', 'loss_rpn_box_reg': '0.10'}\n",
            "[24/25] loss: {'loss_classifier': '0.22', 'loss_box_reg': '0.30', 'loss_objectness': '0.06', 'loss_rpn_box_reg': '0.16'}\n",
            "DATA IS BEING VALIDATED FOR A NEW EPOCH\n",
            "\n",
            "Average precision of this image:  tensor(0.4175)\n",
            "Average precision of this image:  tensor(0.2500)\n",
            "Average precision of this image:  tensor(0.5556)\n",
            "Average precision of this image:  tensor(0.5621)\n",
            "Average precision of this image:  tensor(0.3318)\n",
            "Average precision of this image:  tensor(0.4750)\n",
            "Average precision of this image:  tensor(0.4091)\n",
            "Average precision of this image:  tensor(0.1250)\n",
            "Average precision of this image:  tensor(0.7500)\n",
            "Average precision of this image:  tensor(0.2704)\n",
            "Average precision of this image:  tensor(0.2500)\n",
            "Average precision of this image:  tensor(0.1667)\n",
            "Average precision of this image:  tensor(0.2917)\n",
            "Average precision of this image:  tensor(0.1771)\n",
            "Average precision of this image:  tensor(0.2500)\n",
            "Average precision of this image:  tensor(0.3996)\n",
            "Average precision of this image:  tensor(0.1667)\n",
            "Average precision of this image:  tensor(0.1714)\n",
            "Average precision of this image:  tensor(0.4444)\n",
            "Average precision of this image:  tensor(1.0000)\n",
            "Mean Average precision of this epoch:  0.37320447\n"
          ]
        }
      ],
      "source": [
        "# Optimizer used: SGD\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "# LR scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "if not os.path.exists(\"predictions\"):\n",
        "    os.mkdir(\"predictions\")\n",
        "if not os.path.exists(\"models\"):\n",
        "    os.mkdir(\"models\")\n",
        "\n",
        "def train_eval(json_train, img_train, jsons_p_val, imgs_p_val):\n",
        "    category_list = [\"other vehicle\", \"person\", \"traffic light\", \"traffic sign\",\"truck\", \"train\", \"other person\", \"bus\", \"car\", \"rider\", \"motor\",\"bike\", \"trailer\"]\n",
        "    # Defining hyperparameters:\n",
        "    hparams = {\n",
        "        'num_epochs': 3,\n",
        "        'batch_size': 5,\n",
        "        'channels': 3,\n",
        "        'learning_rate': 0.0001,\n",
        "        'classes': len(category_list),\n",
        "        'nsamples': 25000,\n",
        "        'grid_size': 14\n",
        "    }\n",
        "    use_gpu = True\n",
        "    data_train = DeepDrivingDataset(train=True)\n",
        "    training_dataloader = torch.utils.data.DataLoader(data_train, batch_size=hparams['batch_size'], num_workers=0, collate_fn=collate_fn)\n",
        "\n",
        "    data_test = DeepDrivingDataset(train=False)\n",
        "    testing_dataloader = torch.utils.data.DataLoader(data_test, batch_size=hparams['batch_size'], num_workers=0, collate_fn=collate_fn)\n",
        "    \n",
        "    for epoch in range(hparams['num_epochs']):\n",
        "\n",
        "\n",
        "        total_AP_test = []\n",
        "        print(\"\")\n",
        "\n",
        "        model.train()\n",
        "        train(training_dataloader)\n",
        "        model_name = \"models/\" + name_model + \"_epoch\"+str(epoch)+\".pth\"\n",
        "        torch.save({'model_state_dict': model.state_dict()}, model_name)\n",
        "\n",
        "        model.eval()\n",
        "        if not os.path.exists(\"predictions\"):\n",
        "          os.mkdir(\"predictions\")\n",
        "        \n",
        "        evaluate(testing_dataloader)\n",
        "        folder_pred = \"predictions_epoch_\"+str(epoch)\n",
        "        if not os.path.exists(folder_pred):\n",
        "          os.mkdir(folder_pred)\n",
        "        for file in os.listdir(\"predictions\"):\n",
        "          os.rename(\"predictions/\"+file, folder_pred + \"/\" + file)\n",
        "\n",
        "train_eval(\"DeepDriving/train/labels_TL.json\", \"DeepDriving/train/\", \"DeepDriving/val/labels_TL.json\", \"DeepDriving/val/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performing only the evaluation (loading the models already pretrained with TL)\n",
        "\n"
      ],
      "metadata": {
        "id": "zz9ynwgtal5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section is useful when you do not have enought time to wait for the model be trained. It first downloads the models that are already pretrained (done with the previous section) and then performs only the evaluation for each epoch so it is faster.\n",
        "\n",
        "Downloading the models already pretrained with Transfer Learning..."
      ],
      "metadata": {
        "id": "ERoblB-kbPNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://drive.google.com/file/d/1YaD6ffGgeTCF-SCC4KM6PZkSNJEIZZu_/view?usp=sharing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6RlSLKwbO98",
        "outputId": "d4cfdd0e-c84b-46e4-9723-cb28e49c9223"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-16 05:50:44--  https://drive.google.com/file/d/1YaD6ffGgeTCF-SCC4KM6PZkSNJEIZZu_/view?usp=sharing\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.107.113, 142.251.107.139, 142.251.107.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.107.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘view?usp=sharing’\n",
            "\n",
            "view?usp=sharing        [ <=>                ]  63.93K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2022-03-16 05:50:45 (30.0 MB/s) - ‘view?usp=sharing’ saved [65467]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def only_eval(jsons_p_val, imgs_p_val):\n",
        "    category_list = [\"other vehicle\", \"person\", \"traffic light\", \"traffic sign\",\"truck\", \"train\", \"other person\", \"bus\", \"car\", \"rider\", \"motor\",\"bike\", \"trailer\"]\n",
        "    # Defining hyperparameters:\n",
        "    hparams = {\n",
        "        'num_epochs': 3,\n",
        "        'batch_size': 5,\n",
        "        'channels': 3,\n",
        "        'learning_rate': 0.0001,\n",
        "        'classes': len(category_list),\n",
        "        'nsamples': 25000,\n",
        "        'grid_size': 14\n",
        "    }\n",
        "    use_gpu = True\n",
        "    data_train = DeepDrivingDataset(train=True)\n",
        "    training_dataloader = torch.utils.data.DataLoader(data_train, batch_size=hparams['batch_size'], num_workers=0, collate_fn=collate_fn)\n",
        "\n",
        "    data_test = DeepDrivingDataset(train=False)\n",
        "    testing_dataloader = torch.utils.data.DataLoader(data_test, batch_size=hparams['batch_size'], num_workers=0, collate_fn=collate_fn)\n",
        "    \n",
        "    for epoch in range(hparams['num_epochs']):\n",
        "\n",
        "\n",
        "        total_AP_test = []\n",
        "        print(\"\")\n",
        "\n",
        "        model_name = \"models/\" + name_model + \"_epoch\"+str(epoch)+\".pth\"\n",
        "        checkpoint = torch.load(model_name, map_location=torch.device('cpu'))\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "        model.eval()\n",
        "        if not os.path.exists(\"predictions\"):\n",
        "          os.mkdir(\"predictions\")\n",
        "        \n",
        "        evaluate(testing_dataloader)\n",
        "        folder_pred = \"predictions_epoch_\"+str(epoch)\n",
        "        if not os.path.exists(folder_pred):\n",
        "          os.mkdir(folder_pred)\n",
        "        for file in os.listdir(\"predictions\"):\n",
        "          os.rename(\"predictions/\"+file, folder_pred + \"/\" + file)\n",
        "\n",
        "        only_eval(\"DeepDriving/val/labels_TL.json\", \"DeepDriving/val/\")\n"
      ],
      "metadata": {
        "id": "BIIKIackHGC2"
      },
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Transfer Learning in DeepDriving Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}